{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bbafd2-bc8d-42c3-abaa-3342c8738f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ffmpeg_path = \"/Users/adanenegatarekegn/audio-orchestrator-ffmpeg/bin/ffmpeg\"\n",
    "os.environ['PATH'] += f':{os.path.dirname(ffmpeg_path)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db7912c-5f97-4853-bc81-cdc35dbd6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff440bb-db6b-4861-9f76-ed95ddb3c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_dataset_with_text.py\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import clip\n",
    "import decord\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8988d5-9ba6-4d88-88bb-0bfdebeb2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import clip\n",
    "import decord\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "class GenerateDataset:\n",
    "    def __init__(self, video_path, save_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.clip_model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "\n",
    "        #  BLIP captioning model\n",
    "        self.caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(self.device)\n",
    "\n",
    "        self.dataset = {}\n",
    "        self.video_list = []\n",
    "        self.video_path = ''\n",
    "        self.h5_file = h5py.File(save_path, 'w')\n",
    "\n",
    "        self.set_video_list(video_path)\n",
    "\n",
    "    def set_video_list(self, video_path):\n",
    "        if os.path.isdir(video_path):\n",
    "            self.video_path = video_path\n",
    "            self.video_list = sorted(os.listdir(video_path))\n",
    "            self.video_list = [x for x in self.video_list if x.endswith(('.mp4', '.avi', '.mkv', '.mov'))]\n",
    "        else:\n",
    "            self.video_path = ''\n",
    "            self.video_list.append(video_path)\n",
    "\n",
    "        for idx, file_name in enumerate(self.video_list):\n",
    "            self.dataset['video_{}'.format(idx + 1)] = {}\n",
    "            self.h5_file.create_group('video_{}'.format(idx + 1))\n",
    "\n",
    "    def extract_feature(self, frame):\n",
    "        frame_img = Image.fromarray(frame)\n",
    "        frame_tensor = self.preprocess(frame_img).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.clip_model.encode_image(frame_tensor)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze(0).cpu().numpy()\n",
    "\n",
    "    def generate_caption(self, frame):\n",
    "        frame_img = Image.fromarray(frame)\n",
    "        inputs = self.caption_processor(frame_img, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.caption_model.generate(**inputs)\n",
    "            caption = self.caption_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "\n",
    "    def encode_caption(self, caption_text):\n",
    "        with torch.no_grad():\n",
    "            tokens = clip.tokenize([caption_text]).to(self.device)\n",
    "            features = self.clip_model.encode_text(tokens)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze(0).cpu().numpy()\n",
    "\n",
    "    def get_change_points(self, video_path):\n",
    "        vr = VideoReader(video_path)\n",
    "        fps = int(vr.get_avg_fps())\n",
    "        n_frames = len(vr)\n",
    "        key_indices = vr.get_key_indices()\n",
    "\n",
    "        prev = 0\n",
    "        key_indices_reduced = []\n",
    "        for v in key_indices:\n",
    "            if v - prev > fps:\n",
    "                prev = v\n",
    "                key_indices_reduced.append(v)\n",
    "\n",
    "        key_indices_reduced = [0] + key_indices_reduced + [n_frames]\n",
    "\n",
    "        temp_change_points = []\n",
    "        for idx in range(len(key_indices_reduced) - 1):\n",
    "            segment = [key_indices_reduced[idx], key_indices_reduced[idx + 1] - 1]\n",
    "            temp_change_points.append(segment)\n",
    "\n",
    "        change_points = np.array(temp_change_points)\n",
    "        n_frame_per_seg = np.array([seg[1] - seg[0] + 1 for seg in change_points])\n",
    "\n",
    "        return change_points, n_frame_per_seg\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Convert from video file (mp4) to h5 file with visual features, text features, and captions.\n",
    "        '''\n",
    "        for video_idx, video_filename in enumerate(tqdm(self.video_list, desc='Feature Extract', ncols=80, leave=True)):\n",
    "            video_path = video_filename\n",
    "            if os.path.isdir(self.video_path):\n",
    "                video_path = os.path.join(self.video_path, video_filename)\n",
    "\n",
    "            video_name = os.path.basename(video_path)\n",
    "            vr = VideoReader(video_path, width=224, height=224)  # CLIP expects 224x224\n",
    "\n",
    "            fps = vr.get_avg_fps()\n",
    "            n_frames = len(vr)\n",
    "\n",
    "            visual_feats = []\n",
    "            text_feats = []\n",
    "            captions = []\n",
    "            picks = []\n",
    "            change_points, n_frame_per_seg = self.get_change_points(video_path)\n",
    "\n",
    "            for segment in change_points:\n",
    "                mid = (segment[0] + segment[1]) // 2\n",
    "                frame = vr[mid].asnumpy()\n",
    "\n",
    "                frame_feat = self.extract_feature(frame)\n",
    "                caption = self.generate_caption(frame)\n",
    "                caption_feat = self.encode_caption(caption)\n",
    "\n",
    "                visual_feats.append(frame_feat)\n",
    "                text_feats.append(caption_feat)\n",
    "                captions.append(caption)\n",
    "                picks.append(mid)\n",
    "\n",
    "            # Save to H5\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['video_features'] = np.array(visual_feats)\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['text_features'] = np.array(text_feats)\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['captions'] = np.array(captions, dtype=object)\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['picks'] = np.array(picks)\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['n_frames'] = n_frames\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['fps'] = fps\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['change_points'] = change_points\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['n_frame_per_seg'] = n_frame_per_seg\n",
    "            self.h5_file['video_{}'.format(video_idx + 1)]['video_name'] = video_name\n",
    "\n",
    "        self.h5_file.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636733ac-f108-45eb-b220-b0045b85f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature and caption extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Feature Extract: 100%|██████████████████████████| 50/50 [18:57<00:00, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Saved everything to: /Users/adanenegatarekegn/Documents/1___1_Mulit-modality_VS/output_features/features_with_text.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_folder = \"/Users/adanenegatarekegn/Documents/1___1_VS_Main/3_Datasets_VS_public_custom/TVSum/tvsum_video/\"  # Path to your folder of input videos\n",
    "save_h5_path = \"/Users/adanenegatarekegn/Documents/1___1_Mulit-modality_VS/Dataset_build_check/output_features/features_with_text.h5\"  # Where you want to save\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.dirname(save_h5_path), exist_ok=True)\n",
    "\n",
    "generator = GenerateDataset(video_folder, save_h5_path)\n",
    "generator.generate_dataset()\n",
    "print(f\"Finished! Saved everything to: {save_h5_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3a964-4ea2-4192-a926-d02bee45054c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
